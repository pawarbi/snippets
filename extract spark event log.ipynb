{"cells":[{"cell_type":"markdown","source":["**Extract spark log and analyze**\n","\n","- Pass notebook id, app id, livy id\n","- mount a lakehouse to save the file to a location as log is a zip file thats typically 100s of MB"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"be65c900-8822-4d85-947a-a21bacb4d511"},{"cell_type":"code","source":["def get_spark_app_logs(client, workspace_id, notebook_id, livy_id, app_id, output_path):\n","    \"\"\"\n","    Retrieves Spark application logs from Microsoft Fabric and saves them to the specified path.\n","    \n","    Parameters:\n","    -----------\n","    client : FabricRestClient\n","        An authenticated Fabric REST client\n","    workspace_id : str\n","        The ID of the workspace\n","    notebook_id : str\n","        The ID of the notebook\n","    livy_id : str\n","        The ID of the Livy session\n","    app_id : str\n","        The Spark application ID\n","    output_path : str\n","        Path where the compressed log file will be saved\n","    \n","    Returns:\n","    --------\n","    str\n","        Path to the saved file or error message\n","    \"\"\"\n","    import os\n","       \n","    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/notebooks/{notebook_id}/livySessions/{livy_id}/applications/{app_id}/1/logs\"\n","    \n","    try:\n","        \n","        response = client.get(endpoint)\n","        if response.status_code != 200:\n","            return f\"Error: Received status code {response.status_code}. Response: {response.text}\"\n","        with open(output_path, \"wb\") as f:\n","            f.write(response.content)\n","        \n","        file_size = len(response.content)\n","        return f\"Successfully saved logs ({file_size/1024/1024:.2f} MB) to {output_path}\"\n","    \n","    except Exception as e:\n","        return f\"Error retrieving logs: {str(e)}\"\n","\n","\n","def extract_and_analyze_spark_logs(compressed_log_path, extract_path):\n","    \"\"\"\n","    Extracts and analyzes Spark application logs, saving the extracted content\n","    and providing a basic analysis.\n","\n","    \"\"\"\n","    import os\n","    import json\n","    import zipfile\n","    import gzip\n","    import io\n","    \n","    os.makedirs(extract_path, exist_ok=True)\n","    \n","    results = {\n","        \"success\": False,\n","        \"format\": \"unknown\",\n","        \"extracted_files\": [],\n","        \"log_summary\": {},\n","        \"error\": None\n","    }\n","    \n","    try:\n","        with open(compressed_log_path, \"rb\") as f:\n","            header = f.read(16)       \n","\n","        if header.startswith(b'PK\\x03\\x04'):\n","            results[\"format\"] = \"zip\"\n","            \n","            \n","            with zipfile.ZipFile(compressed_log_path, 'r') as zip_ref:\n","                file_list = zip_ref.namelist()\n","                results[\"extracted_files\"] = file_list\n","                zip_ref.extractall(extract_path)\n","                if file_list:\n","                    log_file_path = os.path.join(extract_path, file_list[0])\n","                    results[\"primary_log\"] = log_file_path\n","                    results[\"log_summary\"] = analyze_spark_log(log_file_path)\n","        \n","        elif header.startswith(b'\\x1f\\x8b\\x08'):\n","            results[\"format\"] = \"gzip\"\n","            with gzip.open(compressed_log_path, 'rb') as f:\n","                decompressed_content = f.read()\n","            extracted_file = os.path.join(extract_path, \"application_log.txt\")\n","            with open(extracted_file, \"wb\") as f:\n","                f.write(decompressed_content)\n","            \n","            results[\"extracted_files\"] = [extracted_file]\n","            results[\"primary_log\"] = extracted_file\n","            results[\"log_summary\"] = analyze_spark_log(extracted_file)\n","        \n","        else:\n","\n","            results[\"format\"] = \"uncompressed\"\n","            results[\"extracted_files\"] = [compressed_log_path]\n","            results[\"primary_log\"] = compressed_log_path\n","            results[\"log_summary\"] = analyze_spark_log(compressed_log_path)\n","        \n","        results[\"success\"] = True\n","        \n","    except Exception as e:\n","        results[\"error\"] = str(e)\n","    \n","    return results\n","\n","\n","def analyze_spark_log(log_file_path):\n","    \"\"\"\n","    Analyzes a Spark log file, extracting key events and information.\n","    Intentionaly limited to 10 executors and jobs\n","\n","    \"\"\"\n","    import json\n","    import os\n","    from datetime import datetime\n","    summary = {\n","        \"file_size_mb\": os.path.getsize(log_file_path) / 1024 / 1024,\n","        \"application\": {},\n","        \"environment\": {},\n","        \"jobs\": [],\n","        \"stages\": [],\n","        \"exceptions\": [],\n","        \"executors\": []\n","    }\n","    \n","    event_counts = {}\n","    \n","    try:\n","        with open(log_file_path, 'r') as f:\n","            line_count = 0\n","            for line in f:\n","                line_count += 1\n","                if line_count % 100000 == 0:\n","                    print(f\"Analyzed {line_count} lines...\")\n","                \n","                try:\n","                    event = json.loads(line.strip())\n","                    event_type = event.get(\"Event\", \"Unknown\")\n","\n","                    event_counts[event_type] = event_counts.get(event_type, 0) + 1\n","\n","                    if event_type == \"SparkListenerApplicationStart\":\n","                        summary[\"application\"] = {\n","                            \"name\": event.get(\"App Name\", \"Unknown\"),\n","                            \"id\": event.get(\"App ID\", \"Unknown\"),\n","                            \"start_time\": event.get(\"Timestamp\", 0),\n","                            \"start_time_formatted\": datetime.fromtimestamp(\n","                                event.get(\"Timestamp\", 0)/1000\n","                            ).strftime('%Y-%m-%d %H:%M:%S') if event.get(\"Timestamp\") else \"Unknown\"\n","                        }\n","\n","                    elif event_type == \"SparkListenerApplicationEnd\":\n","                        summary[\"application\"][\"end_time\"] = event.get(\"Timestamp\", 0)\n","                        end_time = event.get(\"Timestamp\", 0)\n","                        start_time = summary[\"application\"].get(\"start_time\", 0)\n","                        \n","                        if end_time and start_time:\n","                            duration_sec = (end_time - start_time) / 1000\n","                            summary[\"application\"][\"duration_sec\"] = duration_sec\n","                            summary[\"application\"][\"duration_formatted\"] = f\"{duration_sec//60:.0f}m {duration_sec%60:.1f}s\"\n","                            summary[\"application\"][\"end_time_formatted\"] = datetime.fromtimestamp(\n","                                end_time/1000\n","                            ).strftime('%Y-%m-%d %H:%M:%S')\n","\n","                    elif event_type == \"SparkListenerEnvironmentUpdate\":\n","                        if \"JVM Information\" in event:\n","                            summary[\"environment\"][\"jvm\"] = event[\"JVM Information\"]\n","                        if \"Spark Properties\" in event:\n","                            key_props = [\"spark.app.name\", \"spark.driver.memory\", \"spark.executor.memory\", \n","                                        \"spark.executor.cores\", \"spark.executor.instances\", \n","                                        \"spark.serializer\", \"spark.sql.shuffle.partitions\"] #add more as needed\n","                            summary[\"environment\"][\"properties\"] = {\n","                                k: v for k, v in event.get(\"Spark Properties\", {}).items() \n","                                if any(key in k for key in key_props)\n","                            }\n","                    \n","                    # Extract job info\n","                    elif event_type == \"SparkListenerJobEnd\":\n","                        if len(summary[\"jobs\"]) < 10:  # Limit to first 10 jobs\n","                            summary[\"jobs\"].append({\n","                                \"id\": event.get(\"Job ID\"),\n","                                \"result\": event.get(\"Job Result\", {}).get(\"Result\", \"Unknown\"),\n","                                \"completion_time\": event.get(\"Completion Time\")\n","                            })\n","                    \n","                    # Extract stage info\n","                    elif event_type == \"SparkListenerStageCompleted\":\n","                        if len(summary[\"stages\"]) < 10:  # Limit to first 10 stages\n","                            stage_info = event.get(\"Stage Info\", {})\n","                            summary[\"stages\"].append({\n","                                \"id\": stage_info.get(\"Stage ID\"),\n","                                \"name\": stage_info.get(\"Stage Name\"),\n","                                \"tasks\": stage_info.get(\"Number of Tasks\"),\n","                                \"successful_tasks\": stage_info.get(\"Number of Complete Tasks\"),\n","                                \"failed_tasks\": stage_info.get(\"Number of Failed Tasks\", 0)\n","                            })\n","                    \n","                    # Extract executor info\n","                    elif event_type == \"SparkListenerExecutorAdded\":\n","                        if len(summary[\"executors\"]) < 10:  # Limit to first 10 executors\n","                            summary[\"executors\"].append({\n","                                \"id\": event.get(\"Executor ID\"),\n","                                \"added_time\": event.get(\"Timestamp\"),\n","                                \"resources\": event.get(\"Resources\", {})\n","                            })\n","                    \n","                    if \"Exception\" in event_type or any(err in json.dumps(event) for err in [\"Error\", \"error\", \"exception\", \"Exception\", \"Failed\", \"failed\"]):\n","                        if len(summary[\"exceptions\"]) < 20:  # Limit to first 20 exceptions\n","                            summary[\"exceptions\"].append(event)\n","                    \n","                except json.JSONDecodeError:\n","                    continue\n","        \n","        summary[\"event_counts\"] = event_counts\n","        summary[\"total_events\"] = sum(event_counts.values())\n","        \n","    except Exception as e:\n","        summary[\"error\"] = str(e)\n","    \n","    return summary"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7469989e-8352-46ff-9131-35f2a40c51e8","normalized_state":"finished","queued_time":"2025-05-16T20:37:15.7382832Z","session_start_time":"2025-05-16T20:37:15.739205Z","execution_start_time":"2025-05-16T20:37:19.4198703Z","execution_finish_time":"2025-05-16T20:37:19.8494858Z","parent_msg_id":"79a1b0b1-cc28-4092-b444-1556119be0fa"}},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"940917a9-14b9-48c0-9460-72e73b109a13"},{"cell_type":"code","source":["\n","import sempy.fabric as fabric\n","\n","client = fabric.FabricRestClient()\n","\n","workspaceid = \"91eb8bf4-20c3-47d1-bf43-2dd8dafec93f\"\n","notebookid = \"996b5d64-bee7-4aaa-92b6-ff2ba76d0fc8\"\n","livyid = \"fbd4b8a6-0461-4d15-9501-cd2acc4180d2\"\n","appid = \"application_1747317648565_0001\"\n","\n","compressed_log_path = \"/lakehouse/default/Files/spark_logs.zip\"\n","extract_dir = \"/lakehouse/default/Files/spark_logs_extracted\"\n","\n","result = get_spark_app_logs(client, workspaceid, notebookid, livyid, appid, compressed_log_path)\n","print(result)\n","\n","if \"Successfully\" in result:\n","    analysis = extract_and_analyze_spark_logs(compressed_log_path, extract_dir)\n","    \n","    if analysis[\"success\"]:\n","        print(f\"Log analysis complete. Format: {analysis['format']}\")\n","        print(f\"Extracted files: {analysis['extracted_files']}\")       \n","\n","        app_info = analysis[\"log_summary\"][\"application\"]\n","        print(\"\\nApplication Summary:\")\n","        print(f\"Name: {app_info.get('name', 'Unknown')}\")\n","        print(f\"ID: {app_info.get('id', 'Unknown')}\")\n","        print(f\"Duration: {app_info.get('duration_formatted', 'Unknown')}\")\n","\n","        exceptions = analysis[\"log_summary\"][\"exceptions\"]\n","        if exceptions:\n","            print(f\"\\nFound {len(exceptions)} exceptions/errors\")\n","            for i, exc in enumerate(exceptions[:3]): \n","                print(f\"Exception {i+1}: {exc.get('Event', 'Unknown')}\")\n","        else:\n","            print(\"\\nNo exceptions found in logs\")\n","\n","        import json\n","        with open(f\"{extract_dir}/log_analysis.json\", \"w\") as f:\n","            json.dump(analysis[\"log_summary\"], f, indent=2)\n","        print(f\"\\nDetailed analysis saved to {extract_dir}/log_analysis.json\")\n","    else:\n","        print(f\"Error analyzing logs: {analysis['error']}\")\n","else:\n","    print(\"Log retrieval failed, cannot proceed with analysis\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"7469989e-8352-46ff-9131-35f2a40c51e8","normalized_state":"finished","queued_time":"2025-05-16T20:37:15.7753584Z","session_start_time":null,"execution_start_time":"2025-05-16T20:37:19.851048Z","execution_finish_time":"2025-05-16T20:37:47.2634829Z","parent_msg_id":"3e4333fa-d319-4c7a-acd9-0524e9c5e296"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully saved logs (30.08 MB) to /lakehouse/default/Files/spark_logs.zip\nLog analysis complete. Format: zip\nExtracted files: ['application_1747317648565_0001_1']\n\nApplication Summary:\nName: nee_cgov_fbd4b8a6-0461-4d15-9501-cd2acc4180d2\nID: application_1747317648565_0001\nDuration: 45m 41.0s\n\nFound 20 exceptions/errors\nException 1: SparkListenerEnvironmentUpdate\nException 2: SparkListenerTaskStart\nException 3: SparkListenerTaskEnd\n\nDetailed analysis saved to /lakehouse/default/Files/spark_logs_extracted/log_analysis.json\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"b4e9e0e7-6ab0-44d6-aa2b-464bf6dcc0fc"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","language":"Jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"fff97a24-adbd-4ceb-aeb6-25b95d2a5177"}],"default_lakehouse":"fff97a24-adbd-4ceb-aeb6-25b95d2a5177","default_lakehouse_name":"sparklogs","default_lakehouse_workspace_id":"91eb8bf4-20c3-47d1-bf43-2dd8dafec93f"}}},"nbformat":4,"nbformat_minor":5}